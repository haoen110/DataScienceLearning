{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "- 自然语言处理的主要范畴\n",
    "    - 文本朗读（Text to speech）/语音合成（Speech synthesis）\n",
    "    - 语音识别（Speech recognition）\n",
    "    - 中文自动分词（Chinese word segmentation）\n",
    "    - 词性标注（Part-of-speech tagging）\n",
    "    - 句法分析（Parsing）\n",
    "    - 自然语言生成（Natural language generation）\n",
    "    - 文本分类（Text categorization）\n",
    "    - 信息检索（Information retrieval）\n",
    "    - 信息抽取（Information extraction）\n",
    "    - 文字校对（Text-proofing）\n",
    "    - 问答系统（Question answering）\n",
    "        - 给一句人类语言的问句，决定其答案。 典型问题有特定答案 (像是加拿大的首都叫什么?)，但也考虑些开放式问句(像是人生的意义是是什么?)\n",
    "    - 机器翻译（Machine translation）\n",
    "        - 将某种人类语言自动翻译至另一种语言\n",
    "    - 自动摘要（Automatic summarization）\n",
    "        - 产生一段文字的大意，通常用于提供已知领域的文章摘要，例如产生报纸上某篇文章之摘要\n",
    "    - 文字蕴涵（Textual entailment）\n",
    "    - 命名实体识别（Named entity recognition）\n",
    "    \n",
    "    \n",
    "- `NLTK` - 自然语言工具包\n",
    "- `nltk.download()` - 下载相关数据\n",
    "- `nltk.download(path)` - 离线选取数据位置\n",
    "\n",
    "### 1. 分词\n",
    "\n",
    "    import nltk.tokenize as tk\n",
    "    tk.sent_tokenize(文本)->句子列表\n",
    "    tk.word_tokenize(文本)->单词列表   -\\\n",
    "    分词器 = tk.WordPunctTokenizer()   > 略有不同\n",
    "    分词器.tokenize(文本)->单词列表     -/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "# tkn.py\n",
    "import nltk.tokenize as tk\n",
    "\n",
    "doc = \"Are you curious about tokenization? \" \\\n",
    "      \"Let's see how it works! \" \\\n",
    "      \"We need to analyze a couple of sentences \" \\\n",
    "      \"with punctuations to see it in action.\"\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 Are you curious about tokenization?\n",
      " 2 Let's see how it works!\n",
      " 3 We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "tokens = tk.sent_tokenize(doc)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"%2d\" % (i + 1), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 Are\n",
      " 2 you\n",
      " 3 curious\n",
      " 4 about\n",
      " 5 tokenization\n",
      " 6 ?\n",
      " 7 Let\n",
      " 8 's\n",
      " 9 see\n",
      "10 how\n",
      "11 it\n",
      "12 works\n",
      "13 !\n",
      "14 We\n",
      "15 need\n",
      "16 to\n",
      "17 analyze\n",
      "18 a\n",
      "19 couple\n",
      "20 of\n",
      "21 sentences\n",
      "22 with\n",
      "23 punctuations\n",
      "24 to\n",
      "25 see\n",
      "26 it\n",
      "27 in\n",
      "28 action\n",
      "29 .\n"
     ]
    }
   ],
   "source": [
    "tokens = tk.word_tokenize(doc)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"%2d\" % (i + 1), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 Are\n",
      " 2 you\n",
      " 3 curious\n",
      " 4 about\n",
      " 5 tokenization\n",
      " 6 ?\n",
      " 7 Let\n",
      " 8 '\n",
      " 9 s\n",
      "10 see\n",
      "11 how\n",
      "12 it\n",
      "13 works\n",
      "14 !\n",
      "15 We\n",
      "16 need\n",
      "17 to\n",
      "18 analyze\n",
      "19 a\n",
      "20 couple\n",
      "21 of\n",
      "22 sentences\n",
      "23 with\n",
      "24 punctuations\n",
      "25 to\n",
      "26 see\n",
      "27 it\n",
      "28 in\n",
      "29 action\n",
      "30 .\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"%2d\" % (i + 1), token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 词干\n",
    "\n",
    "    import nltk.stem.porter as pt\n",
    "    pt.PorterStemmer() -> 波特词干提取器，偏宽松\n",
    "\n",
    "    import nltk.stem.lancaster as lc\n",
    "    lc.LancasterStemmer() -> 朗卡斯特词干提取器，偏严格\n",
    "\n",
    "    import nltk.stem.snowball as sb\n",
    "    sb.SnowballStemmer() -> 思诺博词干提取器，偏中庸\n",
    "\n",
    "    词干提取器.stem(单词)->词干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table     tabl     tabl     tabl\n",
      "probably  probabl     prob  probabl\n",
      "  wolves     wolv     wolv     wolv\n",
      " playing     play     play     play\n",
      "      is       is       is       is\n",
      "     dog      dog      dog      dog\n",
      "     the      the      the      the\n",
      " beaches    beach    beach    beach\n",
      "grounded   ground   ground   ground\n",
      "  dreamt   dreamt   dreamt   dreamt\n",
      "envision    envis    envid    envis\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem.porter as pt\n",
    "import nltk.stem.lancaster as lc\n",
    "import nltk.stem.snowball as sb\n",
    "\n",
    "words = ['table', 'probably', 'wolves', 'playing',\n",
    "         'is', 'dog', 'the', 'beaches', 'grounded',\n",
    "         'dreamt', 'envision']\n",
    "\n",
    "pt_stemmer = pt.PorterStemmer()\n",
    "lc_stemmer = lc.LancasterStemmer()\n",
    "sb_stemmer = sb.SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    pt_stem = pt_stemmer.stem(word)\n",
    "    lc_stem = lc_stemmer.stem(word)\n",
    "    sb_stem = sb_stemmer.stem(word)\n",
    "    print('%8s %8s %8s %8s' % (word, pt_stem, lc_stem, sb_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 原型\n",
    "\n",
    "- 名词：复数->单数\n",
    "- 动词：分词->原型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table    table    table\n",
      "probably probably probably\n",
      "  wolves     wolf   wolves\n",
      " playing  playing     play\n",
      "      is       is       be\n",
      "     dog      dog      dog\n",
      "     the      the      the\n",
      " beaches    beach    beach\n",
      "grounded grounded   ground\n",
      "  dreamt   dreamt    dream\n",
      "envision envision envision\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem as ns\n",
    "\n",
    "lemmatizer = ns.WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    n_lemma = lemmatizer.lemmatize(word, pos='n')  # 按照名次还原， 动词不受影响\n",
    "    v_lemma = lemmatizer.lemmatize(word, pos='v')  # 按照动词还原，名次不受影响\n",
    "    print('%8s %8s %8s' % (word, n_lemma, v_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 词袋\n",
    "\n",
    "The brown dog is running. The black dog is in the black room. Running in the room is forbidden.\n",
    "\n",
    "1 The brown dog is running\n",
    "\n",
    "2 The black dog is in the black room\n",
    "\n",
    "3 Running in the room is forbidden\n",
    "\n",
    "       the  brown       dog   is   running     black   in    room    forbidden\n",
    "    1   1      1         1    1      1            0     0     0          0\n",
    "    2   1      0         1    1      0            2     1     1          0\n",
    "    3   1      0         0    1      1            0     1     1          1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown dog is running. The black dog is in the black room. Running in the room is forbidden.\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize as tk\n",
    "import sklearn.feature_extraction.text as ft\n",
    "\n",
    "doc = 'The brown dog is running. ' \\\n",
    "      'The black dog is in the black room. ' \\\n",
    "      'Running in the room is forbidden.'\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown dog is running.', 'The black dog is in the black room.', 'Running in the room is forbidden.']\n"
     ]
    }
   ],
   "source": [
    "sentences = tk.sent_tokenize(doc)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1 1]\n",
      " [2 0 1 0 1 1 1 0 2]\n",
      " [0 0 0 1 1 1 1 1 1]]\n",
      "['black', 'brown', 'dog', 'forbidden', 'in', 'is', 'room', 'running', 'the']\n"
     ]
    }
   ],
   "source": [
    "cv = ft.CountVectorizer()\n",
    "bow = cv.fit_transform(sentences).toarray()\n",
    "print(bow)\n",
    "words = cv.get_feature_names()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 词频\n",
    "       apple\n",
    "    1 5/8\n",
    "    2 10/100\n",
    "    词袋矩阵的归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.2        0.2        0.         0.         0.2\n",
      "  0.         0.2        0.2       ]\n",
      " [0.25       0.         0.125      0.         0.125      0.125\n",
      "  0.125      0.         0.25      ]\n",
      " [0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.16666667 0.16666667 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as sp\n",
    "tf = sp.normalize(bow, norm='l1')\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 文档频率\n",
    "\n",
    "$$文档频率=\\frac{含有某个单词的样本数}{总样本数}$$\n",
    "\n",
    "### 7. 逆文档频率\n",
    "$$逆文档=\\frac{总样本数}{含有某个单词的样本数}$$\n",
    "\n",
    "### 8. 词频-逆文档频率(TF-IDF)\n",
    "\n",
    "词频矩阵中的每一个元素乘以相应单词的逆文档频率，其值越大说明该词对样本语义的贡献越大，根据每个词的贡献力度，构建学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.59188659 0.45014501 0.         0.         0.34957775\n",
      "  0.         0.45014501 0.34957775]\n",
      " [0.73130492 0.         0.27808812 0.         0.27808812 0.21596023\n",
      "  0.27808812 0.         0.43192047]\n",
      " [0.         0.         0.         0.53972482 0.41047463 0.31877017\n",
      "  0.41047463 0.41047463 0.31877017]]\n"
     ]
    }
   ],
   "source": [
    "tt = ft.TfidfTransformer()\n",
    "tfidf = tt.fit_transform(bow).toarray()\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.文本分类(主题识别)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['misc.forsale', 'rec.motorcycles', 'rec.sport.baseball', 'sci.crypt', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as sd\n",
    "import sklearn.feature_extraction.text as ft\n",
    "import sklearn.naive_bayes as nb\n",
    "\n",
    "train = sd.load_files('../data/20news', encoding='latin1', shuffle=True, random_state=7)\n",
    "train_data = train.data\n",
    "train_y = train.target\n",
    "categories = train.target_names\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The curveballs of right handed pitchers tend to curve to the left -> rec.sport.baseball\n",
      "Caesar cipher is an ancient form of encryption -> sci.crypt\n",
      "This two-wheeler is really good on slippery roads -> rec.motorcycles\n"
     ]
    }
   ],
   "source": [
    "cv = ft.CountVectorizer()  # 创建词袋\n",
    "train_bow = cv.fit_transform(train_data)\n",
    "tt = ft.TfidfTransformer()\n",
    "train_x = tt.fit_transform(train_bow)\n",
    "model = nb.MultinomialNB()\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "test_data = [\n",
    "    'The curveballs of right handed pitchers tend to curve to the left',\n",
    "    'Caesar cipher is an ancient form of encryption',\n",
    "    'This two-wheeler is really good on slippery roads']\n",
    "test_bow = cv.transform(test_data)\n",
    "test_x = tt.transform(test_bow)\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "for sentence, index in zip(test_data, pred_test_y):\n",
    "    print(sentence, '->', categories[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 性别识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7781973816717019\n",
      "Leonardo -> male\n",
      "Amy -> female\n",
      "Sam -> male\n",
      "Tom -> male\n",
      "Katherine -> female\n",
      "Taylor -> male\n",
      "Susanne -> female\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk.corpus as nc  # 语料库\n",
    "import nltk.classify as cf\n",
    "\n",
    "male_names = nc.names.words('male.txt')\n",
    "female_names = nc.names.words('female.txt')\n",
    "models, acs = [], []\n",
    "\n",
    "for n_letters in range(1, 6):\n",
    "    data = []\n",
    "    for male_name in male_names:\n",
    "        feature = {'feature': male_name[\n",
    "            -n_letters:].lower()}\n",
    "        data.append((feature, 'male'))\n",
    "    for female_name in female_names:\n",
    "        feature = {'feature': female_name[\n",
    "            -n_letters:].lower()}\n",
    "        data.append((feature, 'female'))\n",
    "    random.seed(7)\n",
    "    random.shuffle(data)\n",
    "    train_data = data[:int(len(data) / 2)]\n",
    "    test_data = data[int(len(data) / 2):]\n",
    "    model = cf.NaiveBayesClassifier.train(\n",
    "        train_data)\n",
    "    ac = cf.accuracy(model, test_data)\n",
    "    models.append(model)\n",
    "    acs.append(ac)\n",
    "best_index = np.array(acs).argmax()\n",
    "best_letters = best_index + 1\n",
    "best_model = models[best_index]\n",
    "best_ac = acs[best_index]\n",
    "print(best_letters, best_ac)\n",
    "names, genders = [\n",
    "    'Leonardo', 'Amy', 'Sam', 'Tom', 'Katherine',\n",
    "    'Taylor', 'Susanne'], []\n",
    "for name in names:\n",
    "    feature = {'feature': name[\n",
    "        -best_letters:].lower()}\n",
    "    gender = best_model.classify(feature)\n",
    "    genders.append(gender)\n",
    "for name, gender in zip(names, genders):\n",
    "    print(name, '->', gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735\n",
      "outstanding\n",
      "insulting\n",
      "vulnerable\n",
      "ludicrous\n",
      "uninvolving\n",
      "avoids\n",
      "astounding\n",
      "fascination\n",
      "symbol\n",
      "seagal\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus as nc\n",
    "import nltk.classify as cf\n",
    "import nltk.classify.util as cu\n",
    "\n",
    "pdata = []\n",
    "fileids = nc.movie_reviews.fileids('pos')  # 积极的\n",
    "for fileid in fileids:\n",
    "    feature = {}\n",
    "    words = nc.movie_reviews.words(fileid)\n",
    "    for word in words:\n",
    "        feature[word] = True\n",
    "    pdata.append((feature, 'POSITIVE'))\n",
    "    \n",
    "ndata = []\n",
    "fileids = nc.movie_reviews.fileids('neg')  # 消极的\n",
    "for fileid in fileids:\n",
    "    feature = {}\n",
    "    words = nc.movie_reviews.words(fileid)\n",
    "    for word in words:\n",
    "        feature[word] = True\n",
    "    ndata.append((feature, 'NEGATIVE'))\n",
    "    \n",
    "pnumb, nnumb = int(0.8 * len(pdata)), int(0.8 * len(ndata))  # 设定分区比率\n",
    "train_data = pdata[:pnumb] + ndata[:nnumb]  # 训练集\n",
    "test_data = pdata[pnumb:] + ndata[nnumb:]  # 测试集\n",
    "model = cf.NaiveBayesClassifier.train(train_data)  # 训练模型\n",
    "ac = cu.accuracy(model, test_data)  # 准确率\n",
    "print(ac)\n",
    "tops = model.most_informative_features()  # 最重要的词\n",
    "for top in tops[:10]:  # 去前十个\n",
    "    print(top[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is an amazing movie. -> POSITIVE 0.63\n",
      "This is a dull movie. I would never recommend it to anyone. -> NEGATIVE 0.77\n",
      "The cinematography is pretty great in this movie. -> POSITIVE 0.69\n",
      "The direction was terrible and the story was all over the place. -> NEGATIVE 0.67\n"
     ]
    }
   ],
   "source": [
    "reviews = ['It is an amazing movie.',\n",
    "           'This is a dull movie. I would never recommend it to anyone.',\n",
    "           'The cinematography is pretty great in this movie.',\n",
    "           'The direction was terrible and the story was all over the place.']\n",
    "\n",
    "sents, probs = [], []\n",
    "\n",
    "for review in reviews:\n",
    "    feature = {}\n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        feature[word] = True\n",
    "    pcls = model.prob_classify(feature)\n",
    "    sent = pcls.max()\n",
    "    prob = pcls.prob(sent)\n",
    "    sents.append(sent)\n",
    "    probs.append(prob)\n",
    "    \n",
    "for review, sent, prob in zip(reviews, sents, probs):\n",
    "    print(review, '->', sent, '%.2f' % prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 主题抽取(无监督)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.036*\"spaghetti\" + 0.027*\"made\" + 0.020*\"like\" + 0.020*\"wheat\"'), (1, '0.025*\"cryptographi\" + 0.025*\"spaghetti\" + 0.018*\"italian\" + 0.018*\"practic\"')]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "import nltk.tokenize as tk\n",
    "import nltk.corpus as nc\n",
    "import nltk.stem.snowball as sb\n",
    "import gensim.models.ldamodel as gm\n",
    "import gensim.corpora as gc\n",
    "\n",
    "\n",
    "doc = []\n",
    "with open('../data/topic.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc.append(line[:-1])\n",
    "        \n",
    "tokenizer = tk.RegexpTokenizer(r'\\w+')\n",
    "stopwords = nc.stopwords.words('english')\n",
    "stemmer = sb.SnowballStemmer('english')\n",
    "lines_tokens = []\n",
    "\n",
    "for line in doc:\n",
    "    tokens = tokenizer.tokenize(line.lower())\n",
    "    line_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            token = stemmer.stem(token)\n",
    "            line_tokens.append(token)\n",
    "    lines_tokens.append(line_tokens)\n",
    "    \n",
    "dic = gc.Dictionary(lines_tokens)\n",
    "bow = []\n",
    "for line_tokens in lines_tokens:\n",
    "    row = dic.doc2bow(line_tokens)\n",
    "    bow.append(row)\n",
    "n_topics = 2\n",
    "model = gm.LdaModel(bow, num_topics=n_topics, id2word=dic, passes=25)\n",
    "topics = model.print_topics(num_topics=n_topics, num_words=4)\n",
    "print(topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
