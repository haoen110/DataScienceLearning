{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多线程爬虫\n",
    "\n",
    "### 进程线程回顾\n",
    "\n",
    "- 进程\n",
    "    - 系统中正在运行的一个应用程序\n",
    "    - 1个CPU核心1次只能执行1个进程,其他进程处于非运行状态\n",
    "    - N个CPU核心可同时执行N个任务\n",
    "- 线程\n",
    "    - 进程中包含的执行单元,1个进程可包含多个线程\n",
    "    - 线程可使用所属进程空间(1次只能执行1个线程,阻塞)\n",
    "    - 锁：防止多个线程同时使用共享空间\n",
    "- GIL：全局解释锁\n",
    "    - 执行通行证,仅此1个,拿到了通行证可执行,否则等\n",
    "- 应用场景\n",
    "    - 多进程：大量的密集的计算\n",
    "    - 多线程：I/O密集\n",
    "        - 爬虫：网络I/O\n",
    "\t- 写文件：本次磁盘I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例：使用多线程爬取 百思不得其姐 段子\n",
    "- 爬取目标 ：段子内容\n",
    "- URL ：http://www.budejie.com/\n",
    "- xpath表达式 \n",
    "    - `//div[@class=\"j-r-list-c-desc\"]/a/text()`\n",
    "- 知识点\n",
    "    - 队列(from queue import Queue)\n",
    "        - `put()`\n",
    "        - `get()`\n",
    "        - `Queue.empty()`：是否为空\n",
    "        - `Queue.join()`：如果队列为空,执行其他程序\n",
    "    - 线程(import threading)\n",
    "        - `threading.Thread(target=...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "from queue import Queue\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "class BsSpider:\n",
    "    def __init__(self):\n",
    "        self.baseurl = \"http://www.budejie.com/\"\n",
    "        self.headers = {\"User_Agent\": \"Mozilla/5.0\"}\n",
    "        self.urlQueue = Queue()  # url队列\n",
    "        self.resQueue = Queue()  # 响应队列\n",
    "\n",
    "    # 生成URL队列\n",
    "    def get_url(self):\n",
    "        for num in range(1, 51):\n",
    "            url = self.baseurl + str(num)  # 1是第一页\n",
    "            self.urlQueue.put(url)\n",
    "\n",
    "    # 响应队列\n",
    "    def get_html(self):\n",
    "        while True:\n",
    "            url = self.urlQueue.get()\n",
    "            res = requests.get(url, headers=self.headers)\n",
    "            res.encoding = 'utf-8'\n",
    "            html = res.text\n",
    "            # 放到响应队列\n",
    "            self.resQueue.put(html)\n",
    "            # 清除此任务\n",
    "            self.urlQueue.task_done()\n",
    "\n",
    "    # 解析页面\n",
    "    def get_content(self):\n",
    "        while True:\n",
    "            # 从响应队列中一次获取html源码\n",
    "            html = self.resQueue.get()\n",
    "            parse_html = etree.HTML(html)\n",
    "            r_list = parse_html.xpath('//div[@class=\"j-r-list-c-desc\"]/a/text()')\n",
    "            for r in r_list:\n",
    "                print(r + \"\\n\")\n",
    "            # 清除任务\n",
    "            self.resQueue.task_done()\n",
    "\n",
    "    def main(self):\n",
    "        # 存放所有的线程\n",
    "        thread_list = []\n",
    "        # 获取url队列\n",
    "        self.get_url()\n",
    "        # 创建getpage线程\n",
    "        for i in range(3):\n",
    "            thread_res = threading.Thread(target=self.get_html)\n",
    "            thread_list.append(thread_res)\n",
    "        for i in range(2):\n",
    "            thread_parse = threading.Thread(target=self.get_content)\n",
    "            thread_list.append(thread_parse)\n",
    "        # 所有线程开始干活\n",
    "        for th in thread_list:\n",
    "            th.setDaemon(True)\n",
    "            th.start()\n",
    "        # 如果队列为空，则执行其他程序\n",
    "        self.urlQueue.join()\n",
    "        self.resQueue.join()\n",
    "        print(\"运行结束\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    begin = time.time()\n",
    "    spider = BsSpider()\n",
    "    spider.main()\n",
    "    end = time.time()\n",
    "    print(\"运行时间：\", end - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "- 定义\n",
    "    - HTML或XML的解析器,依赖于lxml\n",
    "- 安装：`python -m pip install beautifulsoup4`\n",
    "- 导模块：`from bs4 import BeautifulSoup`\n",
    "- 使用流程\n",
    "    - 导入模块\n",
    "        - `from bs4 import BeautifulSoup`\n",
    "    - 创建解析对象 \n",
    "        - `soup = BeautifulSoup(html,'lxml')`\n",
    "    - 查找节点对象\n",
    "        - `soup.find_all(name=\"属性值\")`\n",
    "        \n",
    "### 支持的解析库\n",
    "- lxml：BeautifulSoup(html,'lxml')\n",
    "    - 速度快，文档容错能力强\n",
    "- python标准库：BeautifulSoup(html,'html.parser')\n",
    "    - 速度一般\n",
    "- xml解析器：BeautifulSoup(html,'xml')\n",
    "    - 速度快，文档容错能力强\n",
    "    \n",
    "### 节点选择器\n",
    "- 选择节点\n",
    "    - `soup.节点名`：`soup.a、soup.ul`\n",
    "- 获取文本内容\n",
    "    - `soup.节点名.string`\n",
    "- 常用方法：`find_all()`：返回列表\n",
    "    - `r_list = soup.find_all(属性名=\"属性值\")`\n",
    "        - `r_list = soup.find_all(class=\"test\")` # 报错尝试使用class_\n",
    "    - `r_list=soup.find_all(\"节点名\",attrs={\"名\":\"值\"})`\n",
    "        - `r_list=soup.find_all(\"div\",attrs={\"class\":\"test\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<div id=\"text\">哈哈</div>'\n",
    "\n",
    "# 创建解析对象\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# 查找节点\n",
    "r_list = soup.find_all(id=\"text\")\n",
    "print(r_list)\n",
    "for r in r_list:\n",
    "    print(r.get_text())\n",
    "\n",
    "r_list = soup.find_all(\"div\", attrs={'id': \"text\"})\n",
    "print(r_list)\n",
    "\n",
    "####################################\n",
    "html = '''<div class=\"test\">你好</div>\n",
    "<div class=\"test\">再见</div>\n",
    "<div class=\"test2\">\n",
    "    <span>第二次</span>\n",
    "</div>'''\n",
    "\n",
    "# class为test的div的文本内容\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "divs = soup.find_all(\"div\", attrs={\"class\": \"test\"})\n",
    "print(type(divs))\n",
    "for div in divs:\n",
    "    print(div.string)\n",
    "    print(div.get_text())\n",
    "\n",
    "# class为test2的div下的span中的文本内容\n",
    "divs = soup.find_all(\"div\", attrs={\"class\": \"test2\"})\n",
    "for div in divs:\n",
    "    print(div.span.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy框架\n",
    "###  解释\n",
    "- 异步处理框架,可配置和可扩展程度非常高,Python中使用最广泛的爬虫框架\n",
    "\n",
    "### 框架组成\n",
    "- 引擎(Engine) ：整个框架核心\n",
    "- 调度器(Scheduler) ：接受从引擎发过来的URL,入队列\n",
    "- 下载器(Downloader)：下载网页源码,返回给爬虫程序\n",
    "- 项目管道(Item Pipeline) ：数据处理\n",
    "- 下载器中间件(Downloader Middlewares)\n",
    "    - 处理引擎与下载器之间的请求与响应\n",
    "- 蜘蛛中间件(Spider Middlerwares)\n",
    "    - 处理爬虫程序输入响应和输出结果以及新的请求\n",
    "- Item：定义爬取结果的数据结构,爬取的数据会被赋值为Item对象\n",
    "\n",
    "### 运行流程\n",
    "1. Engine开始统揽全局,向Spider索要URL\n",
    "2. Engine拿到url后,给Scheduler入队列\n",
    "3. Schduler从队列中拿出url给Engine,通过Downloader Middlewares给Downloader去下载\n",
    "4. Downloader下载完成,把response给Engine\n",
    "5. Engine把response通过Spider Middlewares给Spider\n",
    "6. Spider处理完成后,\n",
    "    - 把数据给Engine,交给Item Pipeline处理,\n",
    "    - 把新的URL给Engine,重复2-6步\n",
    "7. Scheduler中没有任何Requests请求后,程序结束\n",
    "\n",
    "### Scrapy爬虫项目步骤\n",
    "1. 新建项目\n",
    "    - `scrapy startproject 项目名`\n",
    "2. 明确目标(items.py)\n",
    "3. 制作爬虫程序\n",
    "    - cd XXX/spiders：`scrapy genspider 文件名 域名`\n",
    "4. 处理数据(pipelines.py)\n",
    "5. 配置settings.py\n",
    "6. 运行爬虫项目\n",
    "    - `scrapy crawl 爬虫名`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy项目文件详解\n",
    "- 目录结构\n",
    "        testspider/\n",
    "        ├── scrapy.cfg   #项目基本配置文件,不用改\n",
    "        └── testspider\n",
    "            ├── __init__.py\n",
    "            ├── items.py       # 定义爬取数据的结构\n",
    "            ├── middlewares.py # 下载器中间件和蜘蛛中间件实现\n",
    "            ├── pipelines.py   # 处理数据\n",
    "            ├── settings.py    # 项目全局配置\n",
    "            └── spiders        # 存放爬虫程序\n",
    "                ├── __init__.py\n",
    "                ├── myspider.py\n",
    "                \n",
    "### settings.py配置\n",
    "      # 是否遵守robots协议,该为False\n",
    "      ROBOTSTXT_OBEY = False\n",
    "\n",
    "      # 最大并发量,默认为16个\n",
    "      CONCURRENT_REQUESTS = 32\n",
    "\n",
    "      # 下载延迟时间为3秒\n",
    "      DOWNLOAD_DELAY = 3\n",
    "\n",
    "      # 请求报头\n",
    "      DEFAULT_REQUEST_HEADERS = {\n",
    "        'User-Agent': \"Mozilla/5.0\",\n",
    "         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "         'Accept-Language': 'en',\n",
    "        }\n",
    "\n",
    "      # 蜘蛛中间件\n",
    "      SPIDER_MIDDLEWARES = {\n",
    "         'testspider.middlewares.TestspiderSpiderMiddleware': 543,\n",
    "        }\n",
    "\n",
    "      # 下载器中间件\n",
    "      DOWNLOADER_MIDDLEWARES = {\n",
    "         'testspider.middlewares.TestspiderDownloaderMiddleware': 543,\n",
    "        }\n",
    "\n",
    "      # 管道文件\n",
    "      ITEM_PIPELINES = {\n",
    "         'testspider.pipelines.TestspiderPipeline': 300,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例：抓取百度首页源码,存到baidu.html中\n",
    "1. `scrapy startproject baidu`\n",
    "2. `cd baidu/baidu`\n",
    "3. `subl items.py(此示例可不用操作)`\n",
    "4. `cd spiders`\n",
    "5. `scrapy genspider baiduspider baidu.com`\n",
    "        #爬虫名\n",
    "        #域名\n",
    "        #start_urls\n",
    "        def parse(self,response):\n",
    "            with open(\"baidu.html\",\"w\") as f:\n",
    "                f.write(response.text)\n",
    "6. `subl settings.py`\n",
    "    - 关闭robots协议\n",
    "    - 添加Headers\n",
    "7. `cd spiders`\n",
    "8. `scrapy crawl baiduspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### pycharm运行scrapy项目\n",
    "1. 创建文件begin.py和scrapy.cfg同目录\n",
    "        from scrapy import cmdline\n",
    "        cmdline.execute(\"scrapy crawl baiduspider\".split())\n",
    "2. Editconfigurations -> + -> python\n",
    "        name : spider\n",
    "        Script : begin.py的路径\n",
    "        working directory : 你自己的项目路径\n",
    "3. 点击运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器\n",
    "1. yield作用 ：把一个函数当做一个生成器使用\n",
    "2. 斐波那契数列 Fib.py\n",
    "3. yield特点 ：让函数暂停,等待下1次调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n",
      "34\n",
      "55\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# Fib.py\n",
    "def fib(n):\n",
    "    a, b, s = 0, 1, 0\n",
    "    while s < n:\n",
    "        a, b = b, a + b\n",
    "        s += 1\n",
    "        yield b\n",
    "\n",
    "print(fib(5).__next__())\n",
    "for i in fib(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目：csdn \n",
    "1. 知识点 yield 、pipelines.py\n",
    "2. 目标\n",
    "    - https://blog.csdn.net/qq_42231391/article/details/83506181\n",
    "    - 标题、发表时间、阅读数\n",
    "    - 步骤\n",
    "        - 创建项目\n",
    "        - 定义数据结构(items.py)\n",
    "        - 创建爬虫程序\n",
    "        - 第3步抓取的数据通过项目管道去处理\n",
    "        - 全局配置\n",
    "        - 运行爬虫程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目：Daomu\n",
    "1. URL ：http://www.daomubiji.com/dao-mu-bi-ji-1\n",
    "2. 目标\n",
    "    - 书名、书的标题、章节名称、章节数量、章节链接\n",
    "3. 步骤\n",
    "    - 创建项目 Daomu\n",
    "    - 改items.py(定义数据结构)\n",
    "    - 创建爬虫文件\n",
    "    - 改pipelines.py(项目管道文件)\n",
    "    - 配置settings.py\n",
    "    - 运行爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 知识点\n",
    "- `extract()`：获取选择器对象中的文本内容\n",
    "    - `response.xpath('.../text()')` 得到选择器对象(节点文本) `[<selector ...,data='文本内容'>]`\n",
    "    - `extract()` 把选择器对象中的文本取出来 `['文本内容']`\n",
    "- 爬虫程序中的 start_urls必须为列表\n",
    "    - `start_urls = []`\n",
    "- pipelines.py中必须有1个函数叫\n",
    "    - `process_item(self,item,spider)`，当然还可以写任何其他函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 存入MongoDB数据库\n",
    "- 在settings.py中定义相关变量\n",
    "    - `MONGODB_HOST = `\n",
    "    - `MONGODB_PORT = `\n",
    "- 可在pipelines.py中新建一个class\n",
    "        from Daomu import settings\n",
    "        class DaomumongoPipeline(object):\n",
    "            def __init__(self):\n",
    "            host = settings.MONGODB_HOST\n",
    "- 在settings.py文件中设置你的项目管道\n",
    "        ITEM_PIPELINES = {\n",
    "          \"Daomu.pipelines.DaomumongoPipeline\":100,\n",
    "          }\n",
    "#### 存入MySQL数据库\n",
    "- `self.db.commit()`\n",
    "- `Csdn项目存到mongodb和mysql`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 腾讯招聘网站案例\n",
    "- URL\n",
    "    - 第1页：`https://careers.tencent.com/search.html?index=1`\n",
    "    - 第2页：`https://careers.tencent.com/search.html?index=2`\n",
    "- Xpath匹配\n",
    "    - 基准xpath表达式(每个职位节点对象)\n",
    "    - `//div[@class=\"search-content\"]`\n",
    "        - 职位名称：`.//h4/text()`\n",
    "        - 工作地点：`.//span[2]/text()`\n",
    "        - 职位类别：`.//span[3]/text()`\n",
    "        - 发布时间：`.//span[4]/text()`\n",
    "        - 详情信息：`.//p[2]/text()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置手机抓包\n",
    "    - Fiddler(设置抓包)\n",
    "    - 在手机上安装证书\n",
    "        - 手机浏览器打开：http://IP地址:8888 (IP地址是你电脑的IP,8888是Fiddler设置的端口)\n",
    "        - 在页面上下载(FiddlerRoot certificate)\n",
    "        - 下载文件名：FiddlerRoot.cer\n",
    "        0 直接安装\n",
    "    - 设置代理\n",
    "        - 打开手机上已连接的无线, 代理设置 -> 改成 手动\n",
    "        - IP地址：你电脑的IP (ipconfig / ifconfig)\n",
    "        - 端口号：8888"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
