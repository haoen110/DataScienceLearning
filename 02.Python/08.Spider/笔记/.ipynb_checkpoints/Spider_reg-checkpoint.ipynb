{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解析\n",
    "---\n",
    "### 数据的分类\n",
    "- 结构化数据\n",
    "    - 有固定的格式，如 ：HTML、XML、JSON\n",
    "- 非结构化数据\n",
    "    - 图片、音频、视频，这类数据一般都存储为二进制\n",
    "    \n",
    "# 正则表达式 re\n",
    "---\n",
    "- 使用流程\n",
    "    1. 创建编译对象：p = re.compile(\"正则表达式\")\n",
    "    2. 对字符串匹配：r = p.match(\"字符串\")\n",
    "    3. 获取匹配结果：print(r.group())\n",
    "    \n",
    "    \n",
    "- 常用方法\n",
    "    1. match(s) ：字符串开头的第1个,返回对象\n",
    "    2. search(s)：从开始往后找,匹配第1个,返回对象\n",
    "    3. group()  ：从match或search返回对象中取值\n",
    "    4. findall()：所有全部匹配,返回一个列表\n",
    "    \n",
    "    \n",
    "- 表达式\n",
    "        - .  匹配任意字符(不能匹配\\n)\n",
    "        - \\d 数字\n",
    "        - \\s 空白字符\n",
    "        - \\S 非空白字符  \n",
    "        - [...] 包含[]内容 ：A[BCD]E  --> ABE  ACE  ADE \n",
    "        - \\w 字母、数字、_\n",
    "        \n",
    "        - *  0次或多次\n",
    "        - ?  0次或1次\n",
    "        - +  1次或多次\n",
    "        - {m} m次\n",
    "        - {m,n} m-n次  AB{1,3}C --> ABC ABBC ABBBC\n",
    "        \n",
    "        - 贪婪匹配(.*) ：在整个表达式匹配成功的前提下,尽可能多的匹配*\n",
    "        - 非贪婪匹配(.*?) ：在整个表达式匹配成功的前提下,尽可能少的匹配*\n",
    "        \n",
    "        - 分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "贪婪匹配： ['<div><p>春眠不觉晓，处处闻啼鸟</div></p>\\n<div><p>举头望明月，低头思故乡</div></p>']\n",
      "非贪婪匹配： ['<div><p>春眠不觉晓，处处闻啼鸟</div></p>', '<div><p>举头望明月，低头思故乡</div></p>']\n"
     ]
    }
   ],
   "source": [
    "# 贪婪匹配和非贪婪匹配\n",
    "import re\n",
    "\n",
    "s = \"\"\"<div><p>春眠不觉晓，处处闻啼鸟</div></p>\n",
    "<div><p>举头望明月，低头思故乡</div></p>\"\"\"\n",
    "\n",
    "# 创建编译对象\n",
    "p = re.compile('<div><p>.*</div></p>', re.S) # re.S：使.能够匹配\\n在内的所有字符，相当于在中间/s/S\n",
    "p2 = re.compile('<div><p>.*?</div></p>', re.S)\n",
    "# 匹配字符串s\n",
    "r = p.findall(s)\n",
    "r2 = p2.findall(s)\n",
    "print(\"贪婪匹配：\", r)\n",
    "print(\"非贪婪匹配：\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A B', 'C D']\n",
      "['A', 'C']\n",
      "[('A', 'B'), ('C', 'D')]\n"
     ]
    }
   ],
   "source": [
    "# findall()分组示例\n",
    "# 解释：先按照整体匹配出来，然后再匹配()中的，如果有两个或者多个括号，则以元祖的方式显示\n",
    "import re\n",
    "\n",
    "s = \"A B C D\"\n",
    "p1 = re.compile(\"\\w+\\s+\\w+\")\n",
    "print(p1.findall(s))\n",
    "\n",
    "p2 = re.compile(\"(\\w+)\\s+\\w+\")\n",
    "# 第一步：['A B', 'C D']\n",
    "# 第二步：在A B，C D中匹配括号中的内容\n",
    "print(p2.findall(s))\n",
    "\n",
    "p3 = re.compile(\"(\\w+)\\s+(\\w+)\")\n",
    "print(p3.findall(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tiger', '\\n    Two tigers two tigers run fast\\n  '), ('Rabbit', '\\n    Small white rabbit white and white \\n  ')]\n",
      "动物名称： Tiger\n",
      "动物描述： Two tigers two tigers run fast\n",
      "动物名称： Rabbit\n",
      "动物描述： Small white rabbit white and white\n"
     ]
    }
   ],
   "source": [
    "# 练习\n",
    "# 打印：\n",
    "# [(\"Tiger\", \"Two tiger...\"), (\"Rabbit\", \"Small Ra...\")]\n",
    "# 动物名称：Tiger\n",
    "# 动物描述：....\n",
    "\n",
    "import re\n",
    "\n",
    "s = \"\"\"\\\n",
    "<div class=\"animal\">\n",
    "  <p class=\"name\">\n",
    "    <a title=\"Tiger\"></a>\n",
    "  </p>\n",
    "\n",
    "  <p class=\"contents\">\n",
    "    Two tigers two tigers run fast\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"animal\">\n",
    "  <p class=\"name\">\n",
    "    <a title=\"Rabbit\"></a>\n",
    "  </p>\n",
    "\n",
    "  <p class=\"contents\">\n",
    "    Small white rabbit white and white \n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "p = re.compile(r'<div class.*?title=\"(.*?)\">.*?contents\">(.*?)</p>', re.S)\n",
    "r = p.findall(s)\n",
    "print(r)\n",
    "for animal in r:\n",
    "    print(\"动物名称：\", animal[0].strip())\n",
    "    print(\"动物描述：\", animal[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例1：内涵段子脑筋急转弯抓取\n",
    "- 网址 ：www.neihan8.com\n",
    "- 步骤：\n",
    "    1. 找URL规律\n",
    "        - 第1页:https://www.neihanba.com/dz/\n",
    "        - 第2页:https://www.neihanba.com/dz/list_2.html\n",
    "        - 第3页:https://www.neihanba.com/dz/list_3.html\n",
    "    2. 用正则匹配题目、内容\n",
    "        - `p = re.compile('<h4> <a href=.*?<b>(.*?)</b>.*?f18 mb20\">(.*?)</div>', re.S)`\n",
    "    3. 写代码\n",
    "        - 发请求\n",
    "        - 用正则匹配\n",
    "        - 写入本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "\n",
    "class NeiHanSpider:\n",
    "    def __init__(self):\n",
    "        self.baseurl = \"https://www.neihanba.com/dz/\"\n",
    "        self.headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\"}\n",
    "        self.page = 1\n",
    "\n",
    "    # 下载页面\n",
    "    def load_page(self, url):\n",
    "        req = urllib.request.Request(url, headers=self.headers)\n",
    "        res = urllib.request.urlopen(req)\n",
    "        html = res.read().decode(\"gbk\")\n",
    "        self.parse_page(html)\n",
    "\n",
    "    # 解析页面\n",
    "    def parse_page(self, html):\n",
    "        p = re.compile('<h4> <a href=.*?<b>(.*?)</b>.*?f18 mb20\">(.*?)</div>', re.S)\n",
    "        r_list = p.findall(html)\n",
    "        self.write_page(r_list)\n",
    "\n",
    "    # 保存页面\n",
    "    def write_page(self, r_list):\n",
    "        for r_tuple in r_list:\n",
    "            with open(\"dz.txt\", \"a\") as f:\n",
    "                f.write('\\n' + r_tuple[0].strip() + '\\n' + r_tuple[1].strip() + '\\n')\n",
    "\n",
    "    def main(self):\n",
    "        self.load_page(self.baseurl)\n",
    "        while True:\n",
    "            c = input(\"是否继续(y/n)?\")\n",
    "            if c.strip().lower() == 'y':\n",
    "                self.page += 1\n",
    "                url = self.baseurl + \"list_\" + str(self.page) + \".html\"\n",
    "                self.load_page(url)\n",
    "            else:\n",
    "                print(\"爬取结束，谢谢使用！\")\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spider = NeiHanSpider()\n",
    "    spider.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例2：猫眼电影top100榜单,存到csv表格文件中\n",
    "- 网址：猫眼电影 - 榜单 - top100榜\n",
    "- 目标：抓取电影名、主演、上映时间\n",
    "- 知识点讲解\n",
    "    - csv模块的使用流程\n",
    "        - 打开csv文件\n",
    "            - `with open(\"测试.csv\",\"a\") as f:`\n",
    "        - 初始化写入对象\n",
    "            - `writer = csv.writer(f)`\n",
    "        - 写入数据\n",
    "            - `writer.writerow(列表)`\n",
    "    - 示例 见05_csv示例.py\n",
    "        1. 找URL\n",
    "            - 第1页：http://maoyan.com/board/4?offset=0\n",
    "            - 第2页：http://maoyan.com/board/4?offset=10\n",
    "            - 第n页：offset = (n-1)*10\n",
    "        2. 正则匹配\n",
    "            - `<div class=\"movie-item-info\">.*?title=\"(.*?)\".*?<p class=\"star\">(.*?)</p>.*?releasetime\">(.*?)</p>`\n",
    "        3. 写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "class MaoYanSpider:\n",
    "    def __init__(self):\n",
    "        self.baseurl = 'http://maoyan.com/board/4?offset='\n",
    "        self.headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\"}\n",
    "        self.page = 1\n",
    "        self.offset = 0\n",
    "\n",
    "    # 下载页面\n",
    "    def load_page(self, url):\n",
    "        req = urllib.request.Request(url, headers=self.headers)\n",
    "        res = urllib.request.urlopen(req)\n",
    "        html = res.read().decode()\n",
    "        self.parse_page(html)\n",
    "\n",
    "    # 解析页面\n",
    "    def parse_page(self, html):\n",
    "        # p = re.compile('<div class=\"movie-item-info\">.*?title=\"(.*?)\".*?<p class=\"star\">(.*?)</p>.*?releasetime\">(.*?)</p>', re.S)\n",
    "        p = re.compile('<div class=\"movie-item-info\">.*?title=\"(.*?)\".*?主演：(.*?)</p>.*?releasetime\">(.*?)</p>', re.S)\n",
    "        r_list = p.findall(html)\n",
    "        self.write_page(r_list)\n",
    "\n",
    "    # 保存页面\n",
    "    def write_page(self, r_list):\n",
    "        for r_tuple in r_list:\n",
    "            with open(\"top.csv\", \"a\", newline='') as f: # 开头不空行\n",
    "                # 创建写入对象\n",
    "                writer = csv.writer(f)\n",
    "                L = [i.strip() for i in r_tuple]\n",
    "                # [\"霸王别姬\",\"张国荣\",\"1994...\"]\n",
    "                writer.writerow(L)\n",
    "\n",
    "    def main(self):\n",
    "        self.load_page(self.baseurl)\n",
    "        while True:\n",
    "            c = input(\"是否继续(y/n)?\")\n",
    "            if c.strip().lower() == 'y':\n",
    "                self.page += 1\n",
    "                self.offset = (self.page - 1) * 10\n",
    "                url = self.baseurl + str(self.offset)\n",
    "                self.load_page(url)\n",
    "            else:\n",
    "                print(\"爬取结束，谢谢使用！\")\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spider = MaoYanSpider()\n",
    "    spider.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler常用菜单\n",
    "1. Inspector：查看抓到的数据包的详细内容\n",
    "    - 分为请求(request)和响应(response)两部分\n",
    "2. 常用选项\n",
    "    - Headers：显示客户端发送到服务器的header,包含客户端信息、cookie、传输状态\n",
    "    - WebForms：显示请求的POST数据 <body>\n",
    "    - Raw：将整个请求显示为纯文本\n",
    "3. 请求方式及案例\n",
    "    - GET\n",
    "    - POST\n",
    "    - Cookie模拟登陆\n",
    "\n",
    "\n",
    "## 什么是cookie、session\n",
    "- HTTP是一种无连接协议,客户端和服务器交互仅仅限于请求/响应过程,结束后断开,下一次请求时,服务器会认为是一个新的客户端,为了维护他们之间的连接,让服务器知道这是前一个用户发起的请求,必须在一个地方保存客户端信息。\n",
    "    - cookie：通过在客户端记录的信息确定用户身份\n",
    "    - session：通过在服务端记录的信息确定用户身份\n",
    "\n",
    "## 案例3：使用cookie模拟登陆cnblogs\n",
    "    1. 通过抓包工具、F12获取到cookie(先登陆1次网站)\n",
    "\t2. 正常发请求\n",
    "\turl：https://home.cnblogs.com/u/haoenwei/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
