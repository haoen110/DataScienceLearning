{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络爬虫\n",
    "- 定义：网络蜘蛛、网络机器人,抓取网络数据的程序\n",
    "- 总结：用Python程序去模仿人去访问网站,模仿的越逼真越好\n",
    "- 目的：通过有效的大量数据分析市场走势、公司决策\n",
    "\n",
    "### 企业获取数据的方式\n",
    "1. 公司自有数据\n",
    "2. 第三方数据平台购买\n",
    "    - 数据堂、贵阳大数据交易所\n",
    "3. 爬虫爬取数据\n",
    "    - 市场上没有或者价格太高,利用爬虫程序爬取\n",
    "    \n",
    "### Python做爬虫优势\n",
    "- 请求模块、解析模块丰富成熟,强大的scrapy框架\n",
    "    - PHP：对多线程、异步支持不太好\n",
    "    - JAVA：代码笨重,代码量很大\n",
    "    - C/C++：虽然效率高,但是代码成型很慢\n",
    "    \n",
    "### 爬虫分类\n",
    "1. 通用网络爬虫(搜索引擎引用,需要遵守robots协议)\n",
    "    - http://www.taobao.com/robots.txt\n",
    "    - 搜索引擎如何获取一个新网站的URL\n",
    "        - 网站主动向搜索引擎提供(百度站长平台)\n",
    "        - 和DNS服务网(万网),快速收录新网站\n",
    "2. 聚焦网络爬虫\n",
    "    - 自己写的爬虫程序：\n",
    "        - 面向主题的爬虫\n",
    "        - 面向需求的爬虫\n",
    "        \n",
    "### 爬取数据步骤\n",
    "1. 确定需要爬取的URL地址\n",
    "2. 通过HTTP/HTTPS协议来获取相应的HTML页面\n",
    "3. 提取HTML页面有用的数据\n",
    "    - 所需数据，保存\n",
    "    - 页面中有其他的URL，继续第2步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具    \n",
    "### Anaconda和Spyder\n",
    "1. Anaconda：开源的Python发行版本\n",
    "2. Spyder：集成开发环境\n",
    "    - Spyder常用快捷键：\n",
    "    - 注释/取消注释：ctrl + 1\n",
    "    - 保存：ctrl + s\n",
    "    - 运行程序：f5\n",
    "    - 自动补全：Tab\n",
    "    \n",
    "### Chrome浏览器插件\n",
    "- 安装步骤\n",
    "    1. 右上角 - 更多工具 - 扩展程序\n",
    "    2. 点开右上角 - 开发者模式\n",
    "    3. 把插件拖拽到浏览器页面，释放鼠标，点击添加扩展...\n",
    "- 插件介绍\n",
    "    1. Proxy SwitchOmega：代理切换插件\n",
    "    2. Xpath Helper：网页数据解析插件\n",
    "    3. JSON View：查看json格式的数据(好看)\n",
    "    \n",
    "### Fiddler抓包工具\n",
    "1. mac安装\n",
    "    - 安装mono\n",
    "    - 执行 `/Library/Frameworks/Mono.framework/Versions/<Mono Version>/bin/mozroots --import --sync`\n",
    "    - 执行 `export PATH=\"/Library/Frameworks/Mono.framework/Versions/5.20.1/bin:$PATH\"`\n",
    "2. 抓包工具设置\n",
    "    - Tools -> options -> HTTPS -> ...from browers only\n",
    "    - connections：设置端口号 8888\n",
    "3. 设置浏览器代理\n",
    "    - Proxy SwitchOmega -> 选项 -> 新建情景模式 -> HTTP 127.0.0.1 8888 -> 应用选项\n",
    "4. 浏览器右上角图标 -> proxy(self) -> 访问百度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识点\n",
    "### WEB\n",
    "- HTTP和HTTS\n",
    "    - HTTP：80\n",
    "    - HTTPS：443，HTTP的升级版，加了一个安全套接层\n",
    "- GET和POST\n",
    "    - GET：查询参数都会在URL上显示出来\n",
    "    - POST：查询参数和需要提交数据是隐藏在Form表单里的，不会再URL地址上显示\n",
    "- URL：统一资源定位符\n",
    "    - https://  item.jd.com  :80   /26809408972.html #detail\n",
    "    - 协议     域名/IP地址  端口  访问资源的路径    锚点\n",
    "- User-Agent\n",
    "    - 记录用户的浏览器、操作系统等，为了让用户获取更好的HTML页面效果\n",
    "        - User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\n",
    "        - Mozilla Firefox：(Gecko内核)\n",
    "        - IE：Trident(自己的内核)\n",
    "        - Linux：KTHML(like Gecko)\n",
    "        - Apple：Webkit(like KHTML)\n",
    "        - Google：Chrome(like Webkit)\n",
    "        - 其他浏览器都是模仿IE/Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫请求模块\n",
    "- 版本\n",
    "    - python2：urllib2、urllib\n",
    "    - python3：把urllib和urllib2合并,urllib.request\n",
    "- 常用方法\n",
    "    - `urllib.request.urlopen(\"网址\")`\n",
    "        - 作用：向网站发起一个请求并获取响应\n",
    "        - 字节流 = `response.read()`\n",
    "        - 字符串 = `response.read().decode(\"utf-8\")`\n",
    "            - encode() : 字符串 --> bytes\n",
    "            - decode() : bytes  --> 字符串\n",
    "    - 重构User-Agent\n",
    "        - 不支持重构User-Agent：urlopen()\n",
    "        - 支持重构User-Agent\n",
    "            - urllib.request.Request(添加User-Agent)\n",
    "            - urllib.request.Request(\"网址\",headers=\"字典\")\n",
    "            - User-Agent是爬虫和反爬虫斗争的第一步,发送请求**必须**带User-Agent\n",
    "            \n",
    "#### 使用流程(见 02_urllib.request.Request.py)\n",
    "1. 利用Request()方法构建请求对象\n",
    "2. 利用urlopen()方法获取响应对象\n",
    "    - 利用响应对象的read().decode(\"utf-8\")获取内容\n",
    "    - 响应对象response的方法\n",
    "        1. read()：读取服务器响应的内容\n",
    "        2. getcode()：返回HTTP的响应码\n",
    "                print(respones.getcode())\n",
    "                200 ：成功\n",
    "                4XX ：服务器页面出错\n",
    "                5XX ：服务器出错\n",
    "        3. geturl()：返回实际数据的URL(防止重定向问题)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "https://www.baidu.com/\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# 1. 创建请求对象（有User-Agent）\n",
    "url = \"http://www.baidu.com/\"\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\"}\n",
    "\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "\n",
    "# 2. 获取响应对象\n",
    "res = urllib.request.urlopen(req)\n",
    "\n",
    "# 3. 读取响应对象\n",
    "html = res.read().decode(\"utf-8\")\n",
    "\n",
    "# 4. 具体操作\n",
    "# print(html)\n",
    "print(res.getcode())\n",
    "print(res.geturl())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- urllib.parse模块\n",
    "    - urlencode(字典)  ## 给中文编码，三个百分号一个汉子，注意：参数一定要为字典。\n",
    "    - 实例见urlrequest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd=%E7%BE%8E%E5%A5%B3\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "meinv = {\"wd\":\"美女\"}\n",
    "meinv = urllib.parse.urlencode(meinv)\n",
    "print(meinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "# 拼接URL\n",
    "baseurl = \"http://www.baidu.com/s?\"\n",
    "key = input(\"请输入要搜索的内容：\")\n",
    "# 进行urlencode()编码\n",
    "key = urllib.parse.urlencode({\"wd\":key})\n",
    "url = baseurl + key\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\"}\n",
    "\n",
    "# 创建请求对象\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "\n",
    "# 获取响应对象\n",
    "res = urllib.request.urlopen(req)\n",
    "html = res.read().decode(\"utf-8\")\n",
    "\n",
    "# 写入文件\n",
    "with open(\"搜索.html\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quote(字符串编码)\n",
    "- unquote(字符串解码)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  key = urllib.parse.quote(\"字符串\")\n",
    "  baseurl = \"http://www.baidu.com/s?wd=\"\n",
    "  key = input(\"请输入要搜索的内容:\")\n",
    "  #进行quote()编码\n",
    "  key = urllib.parse.quote(key)\n",
    "  url = baseurl + key\n",
    "  print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "- 百度贴吧数据抓取\n",
    "- 要求：\n",
    "    1. 输入要抓取的贴吧名称\n",
    "    2. 输入爬取的起始页和终止页\n",
    "    3. 把每一页的内容保存到本地\n",
    "        - 第1页.html 第2页.html ... ... \n",
    "- 步骤：\n",
    "  1. 找URL规律,拼接URL\n",
    "     - 第1页：http://tieba.baidu.com/f?kw=美女&pn=0\n",
    "     - 第2页：http://tieba.baidu.com/f?kw=美女&pn=50\n",
    "     - 第3页：http://tieba.baidu.com/f?kw=美女&pn=100\n",
    "     - 第n页：pn=(n-1)*50\n",
    "  2. 获取网页内容(发请求获响应)\n",
    "  3. 保存(本地文件、数据库)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请求方式及实例\n",
    "- GET\n",
    "    1. 特点 ：查询参数在URL地址中显示\n",
    "    2. 案例 ：抓取百度贴吧\n",
    "    \n",
    "    \n",
    "- POST(在Request方法中添加data参数)\n",
    "    1. urllib.request.Request(url,data=data,headers=headers)\n",
    "        - data：表单数据以bytes类型提交,不能是str\n",
    "    2. 处理表单数据为bytes类型\n",
    "        1. 把Form表单数据定义为字典data\n",
    "        2. urlencode(data)\n",
    "        3. 转为bytes数据类型：bytes()\n",
    "    3. 有道翻译案例\n",
    "    4. 有道翻译返回的是json格式的字符串,如何把json格式的字符串转换为Python中字典\n",
    "            import json\n",
    "            r_dict = json.loads(r_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入要翻译的内容：天堂\n",
      "heaven\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "\n",
    "# 请输入你要翻译的内容\n",
    "key = input(\"请输入要翻译的内容：\")\n",
    "# 把提交的form表单数据转换为bytes数据类型\n",
    "data = {'i':key,\n",
    "        'from':'AUTO',\n",
    "        'to':'AUTO',\n",
    "        'smartresult':'dict',\n",
    "        'client':'fanyideskweb',\n",
    "        'salt':'15567713651723',\n",
    "        'sign':'3eee1e0b9cbebd65a65007f497a9b33a',\n",
    "        'ts':'1556771365172',\n",
    "        'bv':'d1dc01b5ffc1e7dfd53e6ee3c347fc81',\n",
    "        'doctype':'json',\n",
    "        'version':'2.1',\n",
    "        'keyfrom':'fanyi.web',\n",
    "        'action':'FY_BY_REALTlME'\n",
    "        }\n",
    "# 字符串i=key&from=AUTO&to=AUTO&s....\n",
    "data = urllib.parse.urlencode(data)\n",
    "data = bytes(data, \"utf-8\")\n",
    "# 发请求，获取相应\n",
    "# url为POST地址，抓包工具抓到的\n",
    "url = \"http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule\"\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\"}\n",
    "# 此处data为form表单数据，为bytes数据类型\n",
    "req = urllib.request.Request(url, data=data, headers=headers)\n",
    "res = urllib.request.urlopen(req)\n",
    "r_json = res.read().decode(\"utf-8\")\n",
    "\n",
    "r_dict = json.loads(r_json)\n",
    "print(r_dict[\"translateResult\"][0][0][\"tgt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
